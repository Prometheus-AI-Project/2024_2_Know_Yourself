[
    {
        "question": "우리가 비용 함수의 기울기를 계산하고 그 결과를 벡터 g에 저장했다고 가정합시다. 기울기를 주어진 상태에서 하나의 경사 하강법 업데이트에 드는 비용은 무엇인가요?",
        "choices": [
            "O(D)",
            "O(N)",
            "O(ND)",
            "O(ND^2)"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "문장 1| 연속 확률 변수 x와 그 확률 분포 함수 p(x)에 대해, 모든 x에 대해 0≤p(x)≤1 이 성립한다. 문장 2| 결정 트리는 정보 이득을 최소화하여 학습된다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "문장 1| 연속 확률 변수 x와 그 확률 분포 함수 p(x)에 대해, 모든 x에 대해 0≤p(x)≤1 이 성립한다. 문장 2| 결정 트리는 정보 이득을 최소화하여 학습된다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "아래 주어진 베이지안 네트워크를 고려하십시오. 이 베이지안 네트워크에 필요한 독립적인 파라미터의 수는 얼마인가요? H -> U <- P <- W",
        "choices": [
            "2",
            "4",
            "8",
            "16"
        ],
        "answer": "C",
        "score": 1
    },
    {
        "question": "훈련 예제의 수가 무한대로 증가하면, 해당 데이터로 훈련된 모델은 다음과 같을 것입니다:",
        "choices": [
            "더 낮은 분산",
            "더 높은 분산",
            "같은 분산",
            "위의 어느 것도 아님"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "문장 1| 2D 평면에서 모든 직사각형 집합(축에 정렬되지 않은 직사각형도 포함)은 5개의 점 집합을 깨뜨릴 수 있다. 문장 2| k = 1일 때 k-최근접 이웃 분류기의 VC 차원은 무한하다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "_는 훈련 데이터를 모델링할 수도 없고 새로운 데이터로 일반화할 수도 없는 모델을 나타낸다.",
        "choices": [
            "잘 맞는 모델",
            "과적합",
            "과소적합",
            "위의 모든 항목"
        ],
        "answer": "C",
        "score": 1
    },
    {
        "question": "문장 1| F1 점수는 클래스 불균형이 큰 데이터셋에서 특히 유용할 수 있다. 문장 2| ROC 곡선 아래 면적은 이상 탐지기를 평가하는 데 사용되는 주요 지표 중 하나이다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "문장 1| 역전파 알고리즘은 숨겨진 층을 가진 전역 최적 신경망을 학습한다. 문장 2| 직선의 VC 차원은 최대 2여야 한다. 왜냐하면 3개의 점 중 어떤 직선으로도 분리할 수 없는 경우가 적어도 하나 존재하기 때문이다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "높은 엔트로피는 분류에서 분할이",
        "choices": [
            "순수하다",
            "순수하지 않다",
            "유용하다",
            "쓸모없다"
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "문장 1| 레이어 정규화는 원본 ResNet 논문에서 사용되었고, 배치 정규화는 사용되지 않았다. 문장 2| DCGAN은 훈련 안정화를 위해 자기 주의를 사용한다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "특정 데이터셋에 대해 선형 회귀 모델을 구축할 때, 하나의 특성의 계수가 상대적으로 높은 음수 값을 가짐을 관찰한다. 이는 다음을 시사한다:",
        "choices": [
            "이 특성은 모델에 강한 영향을 미친다(유지해야 함)",
            "이 특성은 모델에 강한 영향을 미치지 않는다(무시해야 함)",
            "이 특성의 중요성에 대해 추가 정보 없이는 언급할 수 없다",
            "무엇도 결정할 수 없다"
        ],
        "answer": "C",
        "score": 1
    },
    {
        "question": "신경망에서, 과소적합(즉, 높은 편향 모델)과 과적합(즉, 높은 분산 모델) 사이의 균형에 가장 큰 영향을 미치는 구조적 가정은 무엇인가?",
        "choices": [
            "숨겨진 노드의 수",
            "학습률",
            "초기 가중치 선택",
            "상수항 입력 유닛 사용"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "다항 회귀에서, 과소적합과 과적합 사이의 균형에 가장 큰 영향을 미치는 구조적 가정은 무엇인가?",
        "choices": [
            "다항식의 차수",
            "가중치를 행렬 역산법 또는 경사 하강법으로 학습할지 여부",
            "가우시안 잡음의 가정된 분산",
            "상수항 입력 유닛 사용"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "문장 1| 2020년 기준으로, 일부 모델은 CIFAR-10에서 98% 이상의 정확도를 달성한다. 문장 2| 원래 ResNet은 Adam 옵티마이저로 최적화되지 않았다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "K-평균 알고리즘은:",
        "choices": [
            "특징 공간의 차원이 샘플 수보다 커서는 안 된다",
            "K = 1일 때 목적 함수의 값이 가장 작다",
            "주어진 클러스터 수에 대해 클래스 내 분산을 최소화한다",
            "초기 중심이 샘플 중 일부로 선택된 경우에만 전역 최적점으로 수렴한다"
        ],
        "answer": "C",
        "score": 1
    },
    {
        "question": "문장 1| VGGNet은 AlexNet의 첫 번째 층 커널보다 더 작은 너비와 높이를 가진 합성곱 커널을 가지고 있다. 문장 2| 데이터 의존적인 가중치 초기화 절차는 배치 정규화 이전에 도입되었다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "다음 행렬의 랭크는 얼마인가? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",
        "choices": [
            "0",
            "1",
            "2",
            "3"
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "문장 1| 밀도 추정(예: 커널 밀도 추정기 사용)은 분류를 수행하는 데 사용할 수 있다. 문장 2| 로지스틱 회귀와 가우시안 나이브 베이즈(동일 클래스 공분산을 가진) 간의 대응 관계는 두 분류기의 매개변수 간에 일대일 대응이 있음을 의미한다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "C",
        "score": 1
    },
    {
        "question": "우리가 집들의 기하학적 위치와 같은 공간 데이터를 클러스터링하려고 한다고 가정합시다. 우리는 크기와 형태가 다른 여러 클러스터를 만들고자 합니다. 다음 방법 중 가장 적합한 방법은 무엇인가요?",
        "choices": [
            "결정 트리",
            "밀도 기반 클러스터링",
            "모델 기반 클러스터링",
            "K-평균 클러스터링"
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "문장 1| AdaBoost에서 잘못 분류된 예제들의 가중치는 동일한 곱셈 인자만큼 증가한다. 문장 2| AdaBoost에서 t번째 약한 분류기의 훈련 데이터에 대한 가중 훈련 오류 e_t는 t에 따라 증가하는 경향이 있다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "MLE 추정치는 종종 바람직하지 않은데 그 이유는 무엇인가요?",
        "choices": [
            "편향이 있다",
            "분산이 크다",
            "일관된 추정자가 아니다",
            "위의 어느 것도 아님"
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "경사 하강법의 계산 복잡도는 무엇인가요?",
        "choices": [
            "D에 대해 선형",
            "N에 대해 선형",
            "D에 대해 다항식",
            "반복 횟수에 의존"
        ],
        "answer": "C",
        "score": 1
    },
    {
        "question": "여러 결정 트리의 출력을 평균내는 것은 _에 도움이 된다.",
        "choices": [
            "편향 증가",
            "편향 감소",
            "분산 증가",
            "분산 감소"
        ],
        "answer": "D",
        "score": 1
    },
    {
        "question": "질병 D의 발생률이 100명 중 5명(즉, P(D) = 0.05)이라고 하자. 부울 랜덤 변수 D는 환자가 '질병 D를 가지고 있다'를 의미하고, 부울 랜덤 변수 TP는 '양성 반응을 나타낸다'를 의미한다. 질병 D에 대한 검사는 매우 정확한 것으로 알려져 있으며, 질병이 있을 때 양성 반응을 보일 확률은 0.99이고, 질병이 없을 때 음성 반응을 보일 확률은 0.97이다. P(TP), 즉 양성 반응을 나타낼 확률은 얼마인가?",
        "choices": [
            "0.0368",
            "0.473",
            "0.078",
            "위의 어느 것도 아님"
        ],
        "answer": "C",
        "score": 1
    },
    {
        "question": "다음 중 결정 트리를 가지치기 하는 주요 이유는 무엇인가요?",
        "choices": [
            "테스트 중 계산 시간을 절약하기 위해",
            "결정 트리를 저장하기 위한 공간을 절약하기 위해",
            "훈련 세트 오류를 더 작게 만들기 위해",
            "훈련 세트의 과적합을 피하기 위해"
        ],
        "answer": "D",
        "score": 1
    },
    {
        "question": "모델이 과적합되고 있다고 가정했을 때, 과적합을 줄이기 위한 유효한 방법이 아닌 것은 무엇인가요?",
        "choices": [
            "훈련 데이터의 양을 증가시킨다.",
            "오류 최소화를 위한 최적화 알고리즘을 개선한다.",
            "모델의 복잡도를 줄인다.",
            "훈련 데이터의 노이즈를 줄인다."
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "문장 1| 소프트맥스 함수는 다중 클래스 로지스틱 회귀에서 일반적으로 사용된다. 문장 2| 비균등 소프트맥스 분포의 온도는 엔트로피에 영향을 미친다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "다음 중 SVM에 대해 참인 것은 무엇인가요?",
        "choices": [
            "2차원 데이터 포인트에 대해, 선형 SVM이 학습한 분리 초평면은 직선이 된다.",
            "이론적으로, 가우시안 커널 SVM은 복잡한 분리 초평면을 모델링할 수 없다.",
            "SVM에서 사용되는 모든 커널 함수에 대해, 등가의 폐쇄형 기저 확장을 얻을 수 있다.",
            "SVM에서 과적합은 지원 벡터의 수와는 관련이 없다."
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "주어진 베이지안 네트워크 H -> U <- P <- W에 의해 설명되는 H, U, P, W의 결합 확률은 무엇인가요? [조건부 확률의 곱으로서]",
        "choices": [
            "P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)",
            "P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)",
            "P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",
            "위의 어느 것도 아님"
        ],
        "answer": "C",
        "score": 1
    },
    {
        "question": "문장 1| ID3 알고리즘은 최적의 결정 트리를 찾을 수 있도록 보장된다. 문장 2| 밀도 함수 f()가 모든 곳에서 0이 아닌 연속 확률 분포를 고려하자. 값 x의 확률은 f(x)와 같다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "N개의 입력 노드와 은닉층이 없고, 하나의 출력 노드가 있는 신경망이 주어졌을 때, 엔트로피 손실 함수와 시그모이드 활성화 함수가 주어졌을 때, 다음 중 적절한 하이퍼파라미터와 초기화로 전역 최적값을 찾을 수 있는 알고리즘은 무엇인가요?",
        "choices": [
            "확률적 경사 하강법",
            "미니배치 경사 하강법",
            "배치 경사 하강법",
            "위의 모든 것"
        ],
        "answer": "D",
        "score": 1
    },
    {
        "question": "Out-of-distribution 탐지를 위한 또 다른 용어는 무엇인가요?",
        "choices": [
            "이상 탐지",
            "일 클래스 탐지",
            "훈련-테스트 불일치 강건성",
            "배경 탐지"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "문장 1| 우리는 약한 학습기 h를 부스팅하여 분류기 f를 학습한다. f의 결정 경계의 함수형은 h와 동일하지만 매개변수가 다르다. (예: h가 선형 분류기라면 f도 선형 분류기이다). 문장 2| 교차 검증을 사용하여 부스팅에서 반복 횟수를 선택할 수 있다. 이 절차는 과적합을 줄이는 데 도움이 될 수 있다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "D",
        "score": 1
    },
    {
        "question": "훈련 데이터셋에서 N이 인스턴스의 수일 때, 최근접 이웃 분류기의 실행 시간은",
        "choices": [
            "O(1)",
            "O( N )",
            "O(log N )",
            "O( N^2 )"
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "문장 1| 원래 ResNet과 Transformer는 모두 피드포워드 신경망이다. 문장 2| 원래 Transformer는 자기 주의를 사용하지만, 원래 ResNet은 사용하지 않는다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "다음 중 공간 클러스터링 알고리즘은 무엇인가요?",
        "choices": [
            "분할 기반 클러스터링",
            "K-평균 클러스터링",
            "격자 기반 클러스터링",
            "위의 모든 것"
        ],
        "answer": "D",
        "score": 1
    },
    {
        "question": "문장 1| 서포트 벡터 머신이 구성하는 최대 여유 결정 경계는 모든 선형 분류기 중에서 가장 낮은 일반화 오류를 가진다. 문장 2| 클래스 조건부 가우시안 분포를 가진 생성 모델에서 얻은 어떤 결정 경계도 원칙적으로 SVM과 다항 커널 차수 3 이하로 재현될 수 있다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "D",
        "score": 1
    },
    {
        "question": "문장 1| 선형 모델의 L2 정규화는 L1 정규화보다 모델을 더 희소하게 만든다. 문장 2| 잔차 연결은 ResNet과 Transformer에서 찾을 수 있다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "D",
        "score": 1
    },
    {
        "question": "배깅을 수행할 때 과적합을 방지하는 것은 무엇인가요?",
        "choices": [
            "샘플링 기법으로 교체를 사용하는 것",
            "약한 분류기를 사용하는 것",
            "과적합에 취약하지 않은 분류 알고리즘을 사용하는 것",
            "훈련된 모든 분류기에 대해 수행되는 검증 절차"
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "다음 행렬의 영공간의 차원은 얼마인가요? A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]",
        "choices": [
            "0",
            "1",
            "2",
            "3"
        ],
        "answer": "C",
        "score": 1
    },
    {
        "question": "서포트 벡터란 무엇인가요?",
        "choices": [
            "결정 경계에서 가장 먼 예제들",
            "SVM에서 f(x)를 계산하는 데 필요한 유일한 예제들",
            "데이터 중심",
            "SVM에서 비제로 가중치 αk를 가진 모든 예제들"
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "훈련 손실이 에폭 수에 따라 증가한다면, 학습 과정에서 발생할 수 있는 문제는 무엇인가요?",
        "choices": [
            "정규화가 너무 낮고 모델이 과적합되고 있다",
            "정규화가 너무 높고 모델이 과소적합되고 있다",
            "단계 크기가 너무 크다",
            "단계 크기가 너무 작다"
        ],
        "answer": "C",
        "score": 1
    },
    {
        "question": "질병 D의 발생률이 100명 중 5명(즉, P(D) = 0.05)이라고 하자. 부울 랜덤 변수 D는 환자가 '질병 D를 가지고 있다'를 의미하고, 부울 랜덤 변수 TP는 '양성 반응을 나타낸다'를 의미한다. 질병 D에 대한 검사는 매우 정확한 것으로 알려져 있으며, 질병이 있을 때 양성 반응을 보일 확률은 0.99이고, 질병이 없을 때 음성 반응을 보일 확률은 0.97이다. P(D | TP), 즉 양성 반응을 나타낼 확률은 얼마인가?",
        "choices": [
            "0.0495",
            "0.078",
            "0.635",
            "0.97"
        ],
        "answer": "C",
        "score": 1
    },
    {
        "question": "K-폴드 교차 검증은",
        "choices": [
            "K에 대해 선형",
            "K에 대해 이차",
            "K에 대해 삼차",
            "K에 대해 지수"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "두 개의 부울 랜덤 변수 A와 B가 주어졌을 때, P(A) = 1/2, P(B) = 1/3, 그리고 P(A | ¬B) = 1/4이면, P(A | B)는 얼마인가?",
        "choices": [
            "1/6",
            "1/4",
            "3/4",
            "1"
        ],
        "answer": "D",
        "score": 1
    },
    {
        "question": "문장 1| 로지스틱 회귀 모델의 가능도(maximum likelihood)를 최대화하면 여러 개의 국소 최적값이 발생한다. 문장 2| 데이터 분포가 알려져 있으면, 나이브 베이즈 분류기가 다른 분류기보다 더 잘할 수 없다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "B",
        "score": 1
    },
    {
        "question": "문장 1| SVM 학습 알고리즘은 객체 함수에 대해 전역 최적 가설을 찾을 수 있도록 보장된다. 문장 2| 방사형 기저 함수 커널을 통해 특성 공간 Q로 매핑된 후, 퍼셉트론은 원래 공간보다 더 나은 분류 성능을 달성할 수 있다(하지만 이를 보장할 수는 없다).",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "A",
        "score": 1
    },
    {
        "question": "문장 1| 훈련 데이터 집합이 작을 때 과적합이 더 발생할 가능성이 높다. 문장 2| 가설 공간이 작을 때 과적합이 더 발생할 가능성이 높다.",
        "choices": [
            "참, 참",
            "거짓, 거짓",
            "참, 거짓",
            "거짓, 참"
        ],
        "answer": "D",
        "score": 1
    }
]